{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from random import randrange\n",
    "import cPickle\n",
    "import time\n",
    "def unpickle(file):\n",
    "    fo = open(file, 'rb')\n",
    "    data = cPickle.load(fo)\n",
    "    fo.close()\n",
    "    return data\n",
    "train_set_1 = unpickle(\"cifar-10-batches-py/data_batch_1\")\n",
    "train_set_2 = unpickle(\"cifar-10-batches-py/data_batch_2\")\n",
    "train_set_3 = unpickle(\"cifar-10-batches-py/data_batch_3\")\n",
    "train_set_4 = unpickle(\"cifar-10-batches-py/data_batch_4\")\n",
    "train_set_5 = unpickle(\"cifar-10-batches-py/data_batch_5\")\n",
    "test_set = unpickle(\"cifar-10-batches-py/test_batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_1=train_set_1['data']\n",
    "Y_train_1=train_set_1['labels']\n",
    "\n",
    "X_test=test_set['data']\n",
    "Y_test=test_set['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the train_set: (10000, 3072)\n",
      "the shape of the test_set:  (10000, 3072)\n"
     ]
    }
   ],
   "source": [
    "print \"the shape of the train_set:\",X_train_1.shape\n",
    "print \"the shape of the test_set: \",X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the one image:  (3072,)\n",
      "the things in the image:  [ 59  43  50 ..., 140  84  72]\n"
     ]
    }
   ],
   "source": [
    "print \"the shape of the one image: \",X_train_1[0].shape\n",
    "print \"the things in the image: \",X_train_1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the size of the image is 3072*1\n",
    "First thing I do is reshaping the size of the image into (3,1024)\n",
    "That means each patch could get (256*3)\n",
    "there is a lot of ways to sperate the image\n",
    "The way how do we choose the patch, no doubt it will change our resalut\n",
    "1.\n",
    "patch1(16*i+j) i<16 j<16 0~256\n",
    "patch2(16*i+j) 16<i<32 j<16 256~512\n",
    "patch3              512~768\n",
    "patch4                    768~1024\n",
    "2.\n",
    "make a loop \n",
    "each patch get 1 pixel \n",
    "3.\n",
    "...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def div_image(image,k):\n",
    "    image=np.reshape(image,(3,1024))\n",
    "    patch={}\n",
    "    for i in range(k):\n",
    "        patch[i]=[]\n",
    "    r=1024/k\n",
    "    for i in range(len(image)):\n",
    "        for m in range(len(image[0])):\n",
    "            index=m/r\n",
    "            patch[index].append(image[i][m])\n",
    "    return patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collect(data_set,k):\n",
    "    patch_list=[]\n",
    "    for i in range(len(data_set)):\n",
    "        image=data_set[i]\n",
    "        patch=div_image(image,k)\n",
    "        for i in range(k):\n",
    "            patch_list.append(patch[i])\n",
    "    return patch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start collecting the patchs of the data_batch_1....\n",
      "finish the part of collecting the patch...\n"
     ]
    }
   ],
   "source": [
    "print \"Start collecting the patchs of the data_batch_1....\"\n",
    "patch_list=collect(X_train_1,4)\n",
    "patch_list=np.asarray(patch_list)\n",
    "print \"finish the part of collecting the patch...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_patch_list=collect(X_test,4)\n",
    "test_patch_list=np.asarray(test_patch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape pf the test_patch_list  (40000, 768)\n"
     ]
    }
   ],
   "source": [
    "print \"shape pf the test_patch_list \",test_patch_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of the patch_list  40000\n",
      "the type of the patch_list  <type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print \"the length of the patch_list \",len(patch_list)\n",
    "print \"the type of the patch_list \",type(patch_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_cluster_center(k,data_set):\n",
    "    centroids=np.zeros(shape=(k,len(data_set[0])))\n",
    "    for i in range(len(data_set[0])):\n",
    "        smaller_i = min(data_set[:,i])\n",
    "        largest_i = max(data_set[:,i])\n",
    "        for index in range(k):\n",
    "            centroids[index][i]=smaller_i + (largest_i-smaller_i)*(index+1)/float(k)\n",
    "    return centroids\n",
    "\n",
    "def dist_Eclud(variable,cluster):\n",
    "    return np.sqrt(np.sum(np.power(variable-cluster,2)))\n",
    "\n",
    "def K_means(data_set,k):\n",
    "    print \"Start K_means of the list of patch\"\n",
    "    centroids=random_cluster_center(k,data_set)\n",
    "#    print \"the length \",len(centroids)\n",
    "    pre_labels=np.zeros(len(data_set))\n",
    "#    k2label=np.zeros(k)\n",
    "    convege=False\n",
    "#    model_centroids=[]\n",
    "    while not convege:\n",
    "        convege=True\n",
    "        for i in range(len(data_set)):\n",
    "            image=data_set[i]\n",
    "            mindis=np.inf\n",
    "            label=-1\n",
    "            for j in range(k):\n",
    "                distance=dist_Eclud(image,centroids[j])\n",
    "                if distance<mindis:\n",
    "                    mindis=distance\n",
    "                    label=j\n",
    "            if pre_labels[i]!=label:\n",
    "                convege=False\n",
    "            pre_labels[i]=label\n",
    "        \n",
    "        for i in range(k):\n",
    "            label_i_images=data_set[pre_labels==i]\n",
    "            mean=label_i_images.mean(axis=0)\n",
    "            centroids[i]=mean\n",
    "    print \"after using the K-means, we could get K cluster for the patch\"\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start creating the dictionnary of patch.....\n",
      "Start K_means of the list of patch\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d0fa6aa923b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Start creating the dictionnary of patch.....\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcentroids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mK_means\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatch_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-5af7eec44350>\u001b[0m in \u001b[0;36mK_means\u001b[0;34m(data_set, k)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0mdistance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdist_Eclud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcentroids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mmindis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0mmindis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-5af7eec44350>\u001b[0m in \u001b[0;36mdist_Eclud\u001b[0;34m(variable, cluster)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdist_Eclud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mK_means\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print \"Start creating the dictionnary of patch.....\"\n",
    "K=200\n",
    "centroids=K_means(patch_list,K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 2 hours, The K_means which is created by myself donot end\n",
    "So, I try to use the K_means which is given by the package sk.learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start creating the dictionary\n",
      "Start using the sklearn Kmeans....\n",
      "K-means has done...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans \n",
    "K=200\n",
    "print \"Start creating the dictionary\"\n",
    "print \"Start using the sklearn Kmeans....\"\n",
    "result = KMeans(n_clusters=K).fit(patch_list)\n",
    "print \"K-means has done...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "centroids=result.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist_Eclud(variable,cluster):\n",
    "    return np.sqrt(np.sum(np.power(variable-cluster,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dictionary(centroids,image,i,n,flag):\n",
    "    patch={}\n",
    "    if (flag==True):\n",
    "        for m in range(n):\n",
    "            patch[m]=patch_list[i*n+m]\n",
    "    if flag==False:\n",
    "        for m in range(n):\n",
    "            patch[m]=test_patch_list[i*n+m]\n",
    "    dic_patch={}\n",
    "    for m in range(n):\n",
    "        dic_patch[m]=np.zeros(len(centroids))\n",
    "    \n",
    "    d={}\n",
    "    for i in range(n):\n",
    "        d[i]=[]\n",
    "    for centroid in centroids:\n",
    "        for i in range(n):\n",
    "            d[i].append(dist_Eclud(patch[i],centroid))\n",
    "\n",
    "    for m in range(n):\n",
    "        index=d[m].index(min(d[m]))\n",
    "        dic_patch[m][index]=1\n",
    "    if (n>=2):\n",
    "        dic=np.concatenate((dic_patch[0],dic_patch[1]),axis=0)\n",
    "    for i in range(2,n):\n",
    "        dic=np.concatenate((dic,dic_patch[i]),axis=0)\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each image,we create a dic.\n",
    "The variable dic include,4 dic_patch\n",
    "Each dic_patch shows the patch belongs to which cluster Nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def image_dictionary(centroids,data_set,flag):\n",
    "    image_dic=[]\n",
    "    for i in range(len(data_set)):\n",
    "        image=data_set[i]\n",
    "        dic=create_dictionary(centroids,image,i,4,flag)\n",
    "        image_dic.append(dic)\n",
    "    image_dic=np.asarray(image_dic)\n",
    "    return image_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dictionary has created.....\n"
     ]
    }
   ],
   "source": [
    "train_dic=image_dictionary(centroids,X_train_1,True)\n",
    "print \"train dictionary has created.....\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the image_dic (10000, 800)\n"
     ]
    }
   ],
   "source": [
    "print \"the shape of the image_dic\",(train_dic).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dic=image_dictionary(centroids,X_test,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the test_dic (10000, 800)\n"
     ]
    }
   ],
   "source": [
    "print \"the shape of the test_dic\",test_dic.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the right input, we could choose to our own classify model or could just choose the keras to do this work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using the OneVsRestClassifier model to build the model....\n",
      "model compeleted\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "print \"using the OneVsRestClassifier model to build the model....\"\n",
    "ovrc=OneVsRestClassifier(LinearSVC(random_state=0))\n",
    "model=ovrc.fit(train_dic,Y_train_1)\n",
    "\n",
    "print \"model compeleted\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3337\n"
     ]
    }
   ],
   "source": [
    "print model.score(test_dic,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
